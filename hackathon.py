# -*- coding: utf-8 -*-
"""Hackathon

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qhqutKnpG10jbaMxCM7OMZKLfOM1S_Jm
"""

!pip install requests beautifulsoup4 spacy python-dateutil --quiet
!python -m spacy download en_core_web_sm --quiet

import requests, re, time, sys, os, json
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from typing import Dict, List, Optional
from IPython.display import display, Markdown
import spacy # Import spaCy for better entity recognition
import google.generativeai as genai # Import the Gemini API library
from google.colab import userdata # Used to access the Colab Secrets Manager

# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("Downloading spaCy model 'en_core_web_sm'...")
    from spacy.cli import download
    download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")


class ArticleFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' # More common User-Agent
        })

    def fetch_article(self, url: str) -> Optional[Dict[str, str]]:
        try:
            print(f"fetching: {url}")
            r = self.session.get(url, timeout=30)
            r.raise_for_status()
            soup = BeautifulSoup(r.content, 'html.parser')
            return {
                'url': url,
                'title': self._extract_title(soup),
                'content': self._extract_content(soup),
                'author': self._extract_author(soup),
                'date': self._extract_date(soup),
                'domain': urlparse(url).netloc
            }
        except requests.exceptions.RequestException as e:
            print(f"HTTP error: {e}")
            return None # Return None on HTTP errors
        except Exception as e:
            print(f"General error during fetch: {e}")
            return self._read_from_file() # Fallback to file read on other errors

    def _extract_title(self, soup):
        for sel in ['h1','title','meta[property="og:title"]','meta[name="twitter:title"]','.headline','.article-title']:
            el = soup.select_one(sel)
            if el:
                if sel.startswith('meta'):
                    txt = el.get('content', '').strip()
                else:
                    txt = el.get_text(strip=True)
                if len(txt) > 15: return txt
        return "No title found"

    def _extract_content(self, soup):
        # Remove potentially noisy elements
        for el in soup(['script','style','nav','header','footer','aside','.sidebar','form','.ad','.social-share']):
            el.decompose()

        content_elements = []
        # Prioritize common article body containers
        for sel in ['article','.article-body','.content','.entry-content','main','[role="main"]','.post-content']:
            elements = soup.select(sel)
            if elements:
                content_elements.extend(elements)

        # If specific containers not found, try all paragraphs within main content areas
        if not content_elements:
            for sel in ['body', '.container', '.site-content']: # Check broader containers for paragraphs
                 container = soup.select_one(sel)
                 if container:
                     content_elements.extend(container.find_all('p'))


        content = ' '.join(el.get_text(strip=True) for el in content_elements)
        content = re.sub(r'\s+', ' ', content).strip()

        # Fallback to all paragraphs if content is still too short
        if len(content) < 200:
             content = ' '.join(p.get_text(strip=True) for p in soup.find_all('p'))
             content = re.sub(r'\s+', ' ', content).strip()


        return content if len(content) > 50 else "Content extraction failed or too short." # Require minimum content length


    def _extract_author(self, soup):
        for sel in ['[rel="author"]','.author','[property="author"]','.byline','meta[name="author"]','meta[property="article:author"]']:
            el = soup.select_one(sel)
            if el:
                if sel.startswith('meta'):
                    return el.get('content', '').strip()
                return el.get_text(strip=True)
        return "Author Unknown"

    def _extract_date(self, soup):
        for sel in ['time[datetime]','[property="article:published_time"]','.date','.published','meta[property="article:published_time"]','meta[name="pubdate"]','.post-date']:
            el = soup.select_one(sel)
            if el:
                date_str = el.get('datetime') or el.get('content') or el.get_text(strip=True)
                # Attempt to parse date and format consistently
                try:
                    from dateutil.parser import parse
                    return parse(date_str).strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    return date_str # Return original if parsing fails
        return "Date Unknown"


    def _read_from_file(self):
        try:
            with open('article.txt','r',encoding='utf-8') as f:
                content = f.read().strip()
            return {'url':'local_file','title':'Local Article (File)',
                    'content':content,
                    'author':'File Source','date':'Unknown',
                    'domain':'local'}
        except FileNotFoundError:
            print("Local file 'article.txt' not found.")
            return None
        except Exception as e:
            print(f"Error reading local file: {e}")
            return None


class SkepticalAnalyzer:
    def __init__(self):
        # Configure the Gemini API with the API key from Colab Secrets
        try:
            GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
            genai.configure(api_key=GOOGLE_API_KEY)
            self.model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using a suitable model
        except Exception as e:
            print(f"Error configuring Gemini API: {e}")
            self.model = None # Set model to None if configuration fails


    def analyze(self, data: Dict[str,str]) -> Dict[str,str]:
        content = data.get('content', '')
        if not content or len(content) < 100:
            return {
                'core_claims': ["Insufficient content to analyze."],
                'language_tone': "Cannot determine due to insufficient content.",
                'red_flags': ["Insufficient content to analyze."],
                'verification_questions': ["Cannot generate questions due to insufficient content."],
                'key_entities': {"people":[],"organizations": [], "locations":[], "dates": []},
                'counter_perspective': "Cannot generate counter perspective due to insufficient content."
            }

        # Use Gemini for nuanced analysis if model is available
        if self.model:
            generative_analysis = self._perform_generative_analysis(data.get('title', 'No Title'), content)
        else:
            print("Warning: Gemini API not configured. Falling back to rule-based analysis for some fields.")
            generative_analysis = {
                'core_claims': self._claims(content), # Fallback to NLP/rule-based
                'language_tone': self._tone(content), # Fallback to NLP/rule-based
                'red_flags': self._red_flags(data), # Fallback to NLP/rule-based
                'verification_questions': self._questions(data), # Fallback to NLP/rule-based
                'counter_perspective': self._counter(content) # Fallback to NLP/rule-based
            }


        # Use spaCy for structured data extraction (entities)
        entity_results = self._entities(content)
        generative_analysis['key_entities'] = entity_results # Combine entity results

        return generative_analysis


    def _build_master_prompt(self, article_title: str, article_content: str) -> str:
        """Creates a single, powerful prompt for the Generative AI."""
        # This is the core of the "Prompt Engineering" skill.
        # Using a concise prompt for a faster model like 1.5 Flash
        return f"""
        Analyze this news article for bias, tone, and factual claims.
        Title: "{article_title}"
        Content: "{article_content[:4000]}"

        Provide a JSON object with these keys:
        - "core_claims": [3-5 main factual claims.]
        - "language_tone": "Brief analysis of tone (e.g., Neutral, Emotional, Sensational)."
        - "red_flags": [List of potential signs of bias or poor reporting.]
        - "verification_questions": [3-4 questions to verify content.]
        - "counter_perspective": "Brief summary from an opposing viewpoint."

        Return ONLY the JSON object.
        """


    def _perform_generative_analysis(self, title: str, content: str) -> Dict:
        """Calls the Generative AI API and parses the JSON response."""
        full_prompt = self._build_master_prompt(title, content)
        print("--- Calling Generative AI for Analysis ---")
        try:
            response = self.model.generate_content(full_prompt)
            # Attempt to extract JSON from the response text, handling potential markdown wrappers
            response_text = response.text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:-3].strip() # Remove ```json and ```
            elif response_text.startswith('```'):
                 response_text = response_text[3:-3].strip() # Remove ``` and ```

            analysis_json = json.loads(response_text)
            return analysis_json
        except Exception as e:
            print(f"Error during Generative AI analysis: {e}")
            # Fallback to rule-based analysis if AI call or parsing fails
            print("Falling back to rule-based analysis.")
            return {
                'core_claims': self._claims(content),
                'language_tone': self._tone(content),
                'red_flags': self._red_flags({'title': title, 'content': content, 'url': 'N/A', 'author': 'N/A', 'date': 'N/A', 'domain': 'N/A'}), # Pass minimal data for fallback
                'verification_questions': self._questions({'title': title, 'content': content, 'url': 'N/A', 'author': 'N/A', 'date': 'N/A', 'domain': 'N/A'}),
                'counter_perspective': self._counter(content)
            }


    # The following methods are now fallbacks or used for structured extraction (entities)
    def _claims(self, text):
        doc = nlp(text)
        claims = []
        claim_indicators = [
            'report', 'find', 'show', 'suggest', 'reveal', 'indicate',
            'demonstrate', 'assert', 'claim', 'state', 'according to'
        ]
        # Look for sentences containing claim indicators or those structured like claims
        for sent in doc.sents:
            sent_text = sent.text.strip()
            if len(sent_text) > 40 and any(indicator in sent_text.lower() for indicator in claim_indicators):
                 claims.append(sent_text)
            if len(claims) >= 7: break # Limit the number of claims
        if not claims:
            # Fallback to extracting declarative sentences
            claims = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 50 and sent.text.strip().endswith('.')][:5]

        return claims if claims else ["No clear core claims identified."]


    def _tone(self, text):
        doc = nlp(text.lower())
        emotive_terms = ['devastating','shocking','incredible','amazing','terrible','horrific','unprecedented','crisis','scandal','bombshell']
        loaded_terms = ['activist','regime','radical','propaganda','bias','slanted','manipulate'] # Added more loaded terms
        sensational_phrases = ['you wont believe','shocking truth','must read','game changer']

        emotive_count = sum(1 for token in doc if token.text in emotive_terms)
        loaded_count = sum(1 for token in doc if token.text in loaded_terms)
        sensational_count = sum(1 for phrase in sensational_phrases if phrase in text.lower())

        total_words = len(doc) # Use spaCy token count
        emotive_ratio = (emotive_count / total_words) * 100 if total_words > 0 else 0
        loaded_ratio = (loaded_count / total_words) * 100 if total_words > 0 else 0

        tone_flags = []
        if emotive_ratio > 0.8 or loaded_ratio > 0.8 or sensational_count > 0:
            tone_flags.append("Significant use of emotional/loaded language or sensationalism.")
        elif emotive_ratio > 0.3 or loaded_ratio > 0.3:
            tone_flags.append("Some use of emotional/loaded language.")
        else:
            tone_flags.append("Appears mostly neutral in tone.")

        return ", ".join(tone_flags) if tone_flags else "Tone assessment inconclusive."


    def _red_flags(self, d):
        flags = []
        content = d.get('content', '').lower()
        title = d.get('title', '').lower()


        if 'anonymous source' in content or 'sources say' in content and 'name' not in content:
            flags.append("Potential reliance on unverified or anonymous sources.")
        if ('study' in content or 'research' in content or 'data' in content) and not any(link in content for link in ['http','doi.org','.pdf']):
            flags.append("Mentions research, studies, or data without providing links or clear citations.")
        if not any(phrase in content for phrase in ['however','but','although','critics argue','on the other hand','alternatively','some sources say']):
            flags.append("Lacks inclusion of counter-arguments or opposing viewpoints.")
        if any(word in title for word in ['shocking','incredible','stunning','unbelievable','viral','epic']):
            flags.append("Sensational or clickbait-style headline detected.")
        if len(content.split()) < 300: # Increased minimum length for depth
            flags.append("Article content appears relatively short, potentially lacking in-depth analysis.")
        if d.get('author') in ["Author Unknown", "Unknown"]:
             flags.append("Author not clearly identified.")
        if d.get('date') in ["Date Unknown", "Unknown"]:
             flags.append("Publication date not clearly identified.")
        if urlparse(d.get('url', '')).scheme not in ['http', 'https']:
            flags.append("Unconventional URL scheme (not http/https).") # Flag potential local files mistaken for URLs

        return flags if flags else ["No significant red flags identified based on automated analysis."]


    def _questions(self, d):
        qs = []
        content = d.get('content', '')
        domain = d.get('domain', '')
        author = d.get('author', '')

        qs.append(f"What is the primary funding source or organizational backing for the domain '{domain}'?")
        qs.append(f"Can the key claims made in this article regarding '{self._topic(content)}' be independently verified through other reputable sources?")
        qs.append("Who are the specific individuals or organizations cited as sources, and what potential biases or conflicts of interest might they have?")
        qs.append("Does the article present alternative perspectives or acknowledge limitations, and what do credible sources with different viewpoints say on this topic?")

        if 'study' in content.lower() or 'research' in content.lower():
            qs.append("Where can the original research paper or study mentioned in the article be accessed and reviewed?")
        if 'expert' in content.lower() or 'professor' in content.lower() or 'doctor' in content.lower():
            qs.append("What are the specific credentials, expertise, and affiliations of the experts or individuals quoted?")
        if author and author not in ["Author Unknown", "Unknown"]:
             qs.append(f"What is the background, publication history, and potential editorial stance of the author '{author}'?")


        return qs[:6] # Limit the number of questions


    def _entities(self, text):
        doc = nlp(text)
        entities = {'people':[],'organizations':[],'locations':[],'dates':[]}
        for ent in doc.ents:
            if ent.label_ == 'PERSON':
                entities['people'].append(ent.text)
            elif ent.label_ == 'ORG':
                entities['organizations'].append(ent.text)
            elif ent.label_ == 'GPE' or ent.label_ == 'LOC':
                entities['locations'].append(ent.text)
            elif ent.label_ == 'DATE':
                 entities['dates'].append(ent.text)


        # Remove duplicates and limit to top entities
        return {k:list(set(v))[:5] for k,v in entities.items()}


    def _counter(self,text):
        topic = self._topic(text)
        if topic and topic != "subject":
             return f"A critical perspective might argue that the article presents a potentially biased or incomplete view on '{topic}', possibly omitting contradictory evidence or alternative interpretations."
        return "A critical perspective might be needed to fully evaluate the article's claims."


    def _topic(self,text):
        doc = nlp(text.lower())
        # Use noun chunks to identify potential topics
        topics = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) < 5 and not any(token.is_stop for token in chunk)]
        if topics:
            # Simple frequency count of noun chunks
            freq = {}
            for topic in topics:
                freq[topic] = freq.get(topic, 0) + 1
            # Return the most frequent noun chunk as the topic
            return max(freq, key=freq.get) if freq else "the subject"
        return "the subject"


class ReportGenerator:
    def report(self, data:Dict[str,str], a:Dict[str,str]) -> str:
        rep = f"# Critical Analysis Report: {data.get('title', 'Untitled Article')}\n\n"
        rep += f"**Source URL:** {data.get('url', 'N/A')}  \n"
        rep += f"**Author:** {data.get('author', 'N/A')}  \n"
        rep += f"**Publication Date:** {data.get('date', 'N/A')}  \n"
        rep += f"**Domain:** {data.get('domain', 'N/A')}\n\n---\n\n"

        rep += "## Summary of Analysis\n\n"
        rep += "### Identified Core Claims:\n" + self._lst(a.get('core_claims', ["No claims identified."]))
        rep += "\n\n### Assessed Tone:\n" + a.get('language_tone', "Tone assessment not available.")
        rep += "\n\n### Potential Red Flags:\n" + self._lst(a.get('red_flags', ["No specific red flags detected by automated analysis."]))

        rep += "\n\n---\n\n## Further Investigation Needed\n"
        rep += "### Key Verification Questions:\n" + self._lst(a.get('verification_questions', ["No verification questions generated."]), num=True)


        rep += "\n\n---\n\n## Extracted Information\n"
        rep += "### Key Entities Mentioned:\n" + self._ents(a.get('key_entities', {}))
        rep += "\n\n### Potential Counter Perspective:\n" + a.get('counter_perspective', "Counter perspective not available.")
        rep += "\n\n---\n\n*Disclaimer: This report is generated by an automated tool and should be used as a starting point for critical evaluation.*"

        return rep

    def _lst(self,items,num=False):
        if not items: return "- None"
        if num: return '\n'.join(f"{i+1}. {x}" for i,x in enumerate(items))
        return '\n'.join(f"- {x}" for x in items)

    def _ents(self, entities):
        out=[]
        for k,v in entities.items():
            if v: out.append(f"**{k.title()}:** {'; '.join(v)}") # Use semicolon for better separation
        return '\n'.join(out) if out else "None identified."

# quick runner for Digital Skeptic AI
article_url = "https://medium.com/predict/just-a-random-article-13fbe8bfc768"
# article_url = "/content/article.txt" # Using the local file as an example

def run_analysis(url: str):
    print("== Digital Skeptic AI ==")
    print("Performing critical analysis of article...\n")

    try:
        fetcher = ArticleFetcher()
        analyzer = SkepticalAnalyzer()
        generator = ReportGenerator()

        data = fetcher.fetch_article(url) if url and url.startswith('http') else fetcher._read_from_file()
        if not data:
            print("Error: Could not fetch or read article data."); return

        # Check content length before proceeding
        if len(data.get('content', "")) < 100:
            print("Warning: Article content is very short. Analysis may be limited.");
            # Provide minimal analysis for short content
            a = analyzer.analyze({"content": ""}) # Pass empty content to get the "insufficient content" message
        else:
            a = analyzer.analyze(data)


        report = generator.report(data, a)

        fname = f"analysis_report_{int(time.time())}.md" # Changed filename for clarity
        with open(fname,'w',encoding='utf-8') as f: f.write(report)

        print(f"\nAnalysis complete. Report saved to -> {fname}\n")
        display(Markdown(report))

    except KeyboardInterrupt:
        print("\nAnalysis interrupted by user.")
    except Exception as e:
        print(f"\nAn unexpected error occurred during analysis: {e}")

# run
run_analysis(article_url)

