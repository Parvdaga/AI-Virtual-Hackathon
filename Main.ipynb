{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 lxml --quiet\n"
      ],
      "metadata": {
        "id": "d3e2X-zoYBd3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, re, time, sys, os, json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from typing import Dict, List, Optional\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "class ArticleFetcher:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "        })\n",
        "\n",
        "    def fetch_article(self, url: str) -> Optional[Dict[str, str]]:\n",
        "        try:\n",
        "            print(f\"fetching: {url}\")\n",
        "            r = self.session.get(url, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.content, 'html.parser')\n",
        "            return {\n",
        "                'url': url,\n",
        "                'title': self._extract_title(soup),\n",
        "                'content': self._extract_content(soup),\n",
        "                'author': self._extract_author(soup),\n",
        "                'date': self._extract_date(soup),\n",
        "                'domain': urlparse(url).netloc\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"err: {e}\")\n",
        "            return self._read_from_file()\n",
        "\n",
        "    def _extract_title(self, soup):\n",
        "        for sel in ['h1','title','[property=\"og:title\"]','.headline','.article-title']:\n",
        "            el = soup.select_one(sel)\n",
        "            if el:\n",
        "                txt = el.get_text(strip=True)\n",
        "                if len(txt) > 10: return txt\n",
        "        return \"No title\"\n",
        "\n",
        "    def _extract_content(self, soup):\n",
        "        for el in soup(['script','style','nav','header','footer','aside']):\n",
        "            el.decompose()\n",
        "        content = \"\"\n",
        "        for sel in ['article','.article-body','.content','.entry-content','main','[role=\"main\"]']:\n",
        "            els = soup.select(sel)\n",
        "            if els:\n",
        "                content = ' '.join(el.get_text(strip=True) for el in els)\n",
        "                if len(content) > 200: break\n",
        "        if len(content) < 200:\n",
        "            content = ' '.join(p.get_text(strip=True) for p in soup.find_all('p'))\n",
        "        return re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "    def _extract_author(self, soup):\n",
        "        for sel in ['[rel=\"author\"]','.author','[property=\"author\"]','.byline']:\n",
        "            el = soup.select_one(sel)\n",
        "            if el: return el.get_text(strip=True)\n",
        "        return \"Unknown\"\n",
        "\n",
        "    def _extract_date(self, soup):\n",
        "        for sel in ['[property=\"article:published_time\"]','time','.date','.published']:\n",
        "            el = soup.select_one(sel)\n",
        "            if el: return el.get('datetime') or el.get_text(strip=True)\n",
        "        return \"Unknown\"\n",
        "\n",
        "    def _read_from_file(self):\n",
        "        try:\n",
        "            with open('article.txt','r',encoding='utf-8') as f:\n",
        "                content = f.read().strip()\n",
        "            return {'url':'local','title':'Local Article','content':content,\n",
        "                    'author':'Unknown','date':'Unknown','domain':'local'}\n",
        "        except: return None\n",
        "\n",
        "\n",
        "class SkepticalAnalyzer:\n",
        "    def analyze(self, data: Dict[str,str]) -> Dict[str,str]:\n",
        "        return {\n",
        "            'core_claims': self._claims(data['content']),\n",
        "            'language_tone': self._tone(data['content']),\n",
        "            'red_flags': self._red_flags(data),\n",
        "            'verification_questions': self._questions(data),\n",
        "            'key_entities': self._entities(data['content']),\n",
        "            'counter_perspective': self._counter(data['content'])\n",
        "        }\n",
        "\n",
        "    def _claims(self, text):\n",
        "        sents = text.split('.')\n",
        "        claims, indicators = [], [\n",
        "            'according to','data shows','study found','research','report shows',\n",
        "            'experts say','announced','confirmed','revealed'\n",
        "        ]\n",
        "        for s in sents[:20]:\n",
        "            s = s.strip()\n",
        "            if len(s) > 30 and any(x in s.lower() for x in indicators):\n",
        "                claims.append(s+'.')\n",
        "                if len(claims) >= 5: break\n",
        "        if not claims:\n",
        "            claims = [s.strip()+'.' for s in sents if len(s.strip()) > 50][:3]\n",
        "        return claims\n",
        "\n",
        "    def _tone(self, text):\n",
        "        emot = ['devastating','shocking','incredible','amazing','terrible','horrific','unprecedented']\n",
        "        loaded = ['activist','regime','radical','scandal','crisis','exclusive']\n",
        "        low = text.lower()\n",
        "        ecount = sum(1 for w in emot if w in low)\n",
        "        lcount = sum(1 for w in loaded if w in low)\n",
        "        ratio = (ecount/ max(len(text.split()),1))*100\n",
        "        if ratio > 0.5 or lcount > 3:\n",
        "            return \"Heavy emotional/loaded terms.\"\n",
        "        elif ratio > 0.2 or lcount > 1:\n",
        "            return \"Some emotional/loaded terms.\"\n",
        "        return \"Mostly neutral.\"\n",
        "\n",
        "    def _red_flags(self, d):\n",
        "        out, txt = [], d['content'].lower()\n",
        "        if 'anonymous source' in txt: out.append(\"Relies on anonymous sources\")\n",
        "        if ('study' in txt or 'research' in txt) and 'http' not in d['content']:\n",
        "            out.append(\"Mentions research/data w/o links\")\n",
        "        if not any(x in txt for x in ['however','but','although','critics','opponents']):\n",
        "            out.append(\"No counter-views included\")\n",
        "        if any(x in d['title'].lower() for x in ['shocking','incredible','stunning']):\n",
        "            out.append(\"Sensational headline\")\n",
        "        if len(d['content'].split()) < 200:\n",
        "            out.append(\"Too short, lacks depth\")\n",
        "        return out\n",
        "\n",
        "    def _questions(self, d):\n",
        "        qs, text = [], d['content']\n",
        "        base = [\n",
        "            f\"Who funds {d['domain']}?\",\n",
        "            f\"Can claims on {self._topic(text)} be checked elsewhere?\",\n",
        "            \"Who are sources and their motives?\",\n",
        "            \"What do opponents say?\"\n",
        "        ]\n",
        "        if 'study' in text.lower(): qs.append(\"Where is the actual study?\")\n",
        "        if 'expert' in text.lower(): qs.append(\"What are expert credentials?\")\n",
        "        return (qs + base)[:4]\n",
        "\n",
        "    def _entities(self, text):\n",
        "        words, ents = text.split(), {'people':[],'organizations':[],'locations':[]}\n",
        "        for i,w in enumerate(words):\n",
        "            if w[0].isupper() and len(w)>2 and i < len(words)-1 and words[i+1][0].isupper():\n",
        "                ent = f\"{w} {words[i+1]}\"\n",
        "                if any(x in ent for x in ['Inc.','Corp.','Institute']):\n",
        "                    ents['organizations'].append(ent)\n",
        "                else: ents['people'].append(ent)\n",
        "        return {k:list(set(v)) for k,v in ents.items()}\n",
        "\n",
        "    def _counter(self,text):\n",
        "        return f\"A skeptic might argue it's one-sided, ignoring nuance about '{self._topic(text)}'\"\n",
        "\n",
        "    def _topic(self,text):\n",
        "        words = text.split()[:100]; freq = {}\n",
        "        stop = {'the','a','and','or','but','in','on','at','to','for','of','with','by','is','are','was','were'}\n",
        "        for w in words:\n",
        "            w = w.lower().strip('.,!?\":;')\n",
        "            if len(w)>3 and w not in stop: freq[w] = freq.get(w,0)+1\n",
        "        return max(freq,key=freq.get) if freq else \"subject\"\n",
        "\n",
        "\n",
        "class ReportGenerator:\n",
        "    def report(self, data:Dict[str,str], a:Dict[str,str]) -> str:\n",
        "        rep = f\"# Report for: {data['title']}\\n\\n\"\n",
        "        rep += f\"**Source:** {data['url']}  \\n**Author:** {data['author']}  \\n**Date:** {data['date']}  \\n**Domain:** {data['domain']}\\n\\n---\\n\\n\"\n",
        "        rep += \"## Core Claims\\n\" + self._lst(a['core_claims'])\n",
        "        rep += \"\\n\\n## Tone\\n\" + a['language_tone']\n",
        "        rep += \"\\n\\n## Red Flags\\n\" + self._lst(a['red_flags'])\n",
        "        rep += \"\\n\\n## Verification Qs\\n\" + self._lst(a['verification_questions'],num=True)\n",
        "        rep += \"\\n\\n---\\n\\n### Entities\\n\" + self._ents(a['key_entities'])\n",
        "        rep += \"\\n\\n### Counter\\n\" + a['counter_perspective']\n",
        "        return rep\n",
        "\n",
        "    def _lst(self,items,num=False):\n",
        "        if not items: return \"- none\"\n",
        "        if num: return '\\n'.join(f\"{i+1}. {x}\" for i,x in enumerate(items))\n",
        "        return '\\n'.join(f\"- {x}\" for x in items)\n",
        "\n",
        "    def _ents(self, entities):\n",
        "        out=[]\n",
        "        for k,v in entities.items():\n",
        "            if v: out.append(f\"**{k.title()}:** {', '.join(v[:3])}\")\n",
        "        return '\\n'.join(out) if out else \"none\"\n"
      ],
      "metadata": {
        "id": "-bNYO-nUZAPF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick runner for Digital Skeptic AI\n",
        "article_url = \"https://medium.com/predict/just-a-random-article-13fbe8bfc768\"\n",
        "\n",
        "def run_analysis(url: str):\n",
        "    print(\"== Digital Skeptic AI ==\")\n",
        "    print(\"thinking critically about articles...\\n\")\n",
        "\n",
        "    try:\n",
        "        fetcher = ArticleFetcher()\n",
        "        analyzer = SkepticalAnalyzer()\n",
        "        generator = ReportGenerator()\n",
        "\n",
        "        data = fetcher.fetch_article(url) if url else fetcher._read_from_file()\n",
        "        if not data:\n",
        "            print(\"couldn't fetch anything\"); return\n",
        "        if len(data.get('content', \"\")) < 100:\n",
        "            print(\"warn: article looks too short\")\n",
        "\n",
        "        a = analyzer.analyze(data)\n",
        "        report = generator.report(data, a)\n",
        "\n",
        "        fname = f\"analysis_{int(time.time())}.md\"\n",
        "        with open(fname,'w',encoding='utf-8') as f: f.write(report)\n",
        "\n",
        "        print(f\"\\nreport saved -> {fname}\\n\")\n",
        "        display(Markdown(report))\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nuser stopped it.\")\n",
        "    except Exception as e:\n",
        "        print(f\"error: {e}\")\n",
        "\n",
        "# run\n",
        "run_analysis(article_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "LYkn3z5FZDSZ",
        "outputId": "6ae2829a-9842-4691-cef3-95d064720739"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Digital Skeptic AI ==\n",
            "thinking critically about articles...\n",
            "\n",
            "fetching: /content/article.txt\n",
            "err: Invalid URL '/content/article.txt': No scheme supplied. Perhaps you meant https:///content/article.txt?\n",
            "\n",
            "report saved -> analysis_1756380489.md\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Report for: Local Article\n\n**Source:** local  \n**Author:** Unknown  \n**Date:** Unknown  \n**Domain:** local\n\n---\n\n## Core Claims\n- Breaking: Revolutionary Study Reveals Shocking Truth About Coffee Consumption\n\nA groundbreaking new study by the Institute for Advanced Nutritional Research has discovered that drinking coffee may be linked to unprecedented health benefits that experts are calling \"absolutely remarkable.\n- Sarah Johnson, lead researcher on the project, \"Our data shows that people who drink 3-4 cups of coffee daily experience a 45% reduction in serious health issues compared to non-coffee drinkers.\n- The research, funded by an anonymous benefactor, utilized cutting-edge methodology that previous studies have failed to implement.\n- Some critics argue that the study's methodology may be flawed, though they have not provided specific objections to the research design.\n- \"\n\nThe Institute for Advanced Nutritional Research plans to publish their full findings next month, though the complete dataset will not be made publicly available due to privacy concerns regarding participant information.\n\n## Tone\nHeavy emotional/loaded terms.\n\n## Red Flags\n- Mentions research/data w/o links\n\n## Verification Qs\n1. Where is the actual study?\n2. What are expert credentials?\n3. Who funds local?\n4. Can claims on coffee be checked elsewhere?\n\n---\n\n### Entities\n**People:** Mark Thompson,, Study Reveals, Revolutionary Study\n**Organizations:** The Institute\n\n### Counter\nA skeptic might argue it's one-sided, ignoring nuance about 'coffee'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7kofM4iHZE-z"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}